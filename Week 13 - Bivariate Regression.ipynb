{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS 3, Week 13: Bivariate Regression\n",
    "Authors: Eric Van Dusen, Andrew Little, William McEachen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from ipywidgets import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from IPython.display import display, Markdown\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models:\n",
    "When we have two continuous variables (one dependent, one independent), we can use *bivariate regression* to determine how closely the two are related. Biviarate regression is used to determine how changes in one variable -- the independent variable, often denoted $X$ -- can predict changes in another, the dependent variable, often denoted $Y$. Bivariate regression relies on a linear model, which follows the form $ Y_i= \\alpha + \\beta X_i$, where $\\alpha$ is the y-intercept and $\\beta$ is the slope. \n",
    "\n",
    "If we assume that the relationship between our variables is not perfect (or, in the real world, if there is some predictable inaccuracy in our measurement), we add an error term $\\epsilon_i$: $ Y_i= \\alpha + \\beta X_i + \\epsilon_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how we might create an equation for two variables, let's consider the relationship between health care spending and life expectancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spend = pd.read_csv('outspend2015.csv')\n",
    "spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the US is a bit of an outlier in this relationship, so to make some of the key concepts clear we will drop the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_noUS = spend[spend['Spending'] < 14]\n",
    "spend_noUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like last week, we can make a scatterpot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Spending', y='Expectancy', data=spend_noUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, it appears that as Spending increases (as we move further to the right on the x-axis), Expectancy also increases. If we wanted to use this data to make future predictions, we could use a linear model to represent the variables' relationship. Below, you can change the slope and intercept of the line to best fit the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_line(slope, intercept):\n",
    "    #The Linear Model\n",
    "    #    def f(x):\n",
    "    #    return intercept*(slope-1)/30*x +intercept\n",
    "    def f(x):\n",
    "        return intercept + slope*x\n",
    "    x = np.arange(4,16)\n",
    "    y_pred = f(x)\n",
    "    points = (zip(spend_noUS.Spending, spend_noUS.Expectancy))\n",
    "    #The line\n",
    "    sns.scatterplot(x='Spending', y='Expectancy', data=spend_noUS)\n",
    "    plt.plot(x,y_pred)\n",
    "    #The actual data \n",
    "    plt.xlabel('Spending')\n",
    "    plt.ylabel('expectancy')\n",
    "    display(Markdown(rf'$\\hat y$= {slope}$X$ + {intercept}:'))\n",
    "    \n",
    "interact(draw_line, slope=(0.0,3), intercept=(65,80))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What line is best?\n",
    "When we are evaulating how \"good\" a line is, we must address the *residuals*, the difference between the real and predicted values of y: $u_i = Y_i - \\hat{Y_i}$. Because every real y value has an associated residual, we need some way to aggregate the residuals if we are to measure the overall quality of a line\n",
    "\n",
    "#### Absolute value\n",
    "One measurement of loss is calculated by adding the absolute value of the residuals together:\n",
    "$$\\sum_{i=1}^n |u_i| = \\sum_{i=1}^n |Y_i - \\hat{Y_i}|$$\n",
    "\n",
    "#### Squared error:\n",
    "Another measurement is the *squared error*, calculated by adding the squared values of the residuals:\n",
    "$$\\sum_{i=1}^n |u_i^2| = \\sum_{i=1}^n (Y_i-\\hat{Y_i})^2$$\n",
    "\n",
    "For either measurement, we want the line that results in the smallest value (indicating that the total difference between the predicted and actual values is small). Below, try to minimize either the absolute or squared loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(slope, intercept):\n",
    "    #The Linear Model\n",
    "    def f(x):\n",
    "        return intercept + slope*x\n",
    "    x = np.arange(4,16)\n",
    "    y_pred = f(x)\n",
    "    display(Markdown(rf'$\\hat y$= {slope}$X$ + {intercept}:'))\n",
    "    #The line\n",
    "    plt.plot(x,y_pred)\n",
    "    #The Data\n",
    "    sns.scatterplot(x='Spending', y='Expectancy', data=spend_noUS)\n",
    "    #Print the loss\n",
    "    print(\"Square Residual Sum:\", sum([(y-f(x))**2 for x,y in zip(spend_noUS.Spending, spend_noUS.Expectancy)]))\n",
    "    print(\"Absolute Residual Sum:\", sum([abs(y-f(x))for x, y in zip(spend_noUS.Spending, spend_noUS.Expectancy)]))\n",
    "    \n",
    "interact(draw_line, slope=(0.0,2), intercept=(65,80), continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the smallest squared error/absolute error you can produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "Statisticians prefer to use the line that minimizes the squared residuals. To find the slope ($\\beta$) and y-intercept ($\\alpha$), the following equations are used:\n",
    "$$\\beta = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sum_{i=1}^n (X_i - \\overline{X})^2}$$\n",
    "$$\\alpha = \\overline{Y}-\\beta\\overline{X}$$\n",
    "*Reminder*: $\\overline{X}$ represents the mean value of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python:\n",
    "To calculate the slope and y-intercept for the linear model of X and Y, use <code>stats.linregress(X, Y)</code>. This returns a LinregressResult which holds the slope, intercept, the associated r and p values, and standard error (we'll cover what those mean next). To access the slope and intercept, you can index into the LinregressResult, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spend_expect_result = stats.linregress(spend_noUS.Spending, spend_noUS.Expectancy)\n",
    "spend_expect_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few cells we will want to plot a line with this slope and intercept on top of our data. Rather than copy and paste, we can \"pull\" the results from our regression with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_slope = spend_expect_result[0]\n",
    "lm_intercept = spend_expect_result[1]\n",
    "print(\"slope: \", lm_slope, \", Intercept = \", lm_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the slope and intercept, we can plot the line of best fit alongside the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Spending', y='Expectancy', data=spend_noUS)\n",
    "plt.plot(spend_noUS.Spending, spend_noUS.Spending*lm_slope + lm_intercept, label='OLS model')\n",
    "plt.xlabel('Health Care Spending')\n",
    "plt.ylabel('Life Expectancy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolating to the Population\n",
    "Once we have produced slope and intercept values for our model, we wish to know the potential applicability to the population at large. To do this, we build on our previous lectures about hypothesis testing.\n",
    "\n",
    "Here is a bit more about how this is done than we will do in lectures. What matters in the end is knowing how to interpet the model output. \n",
    "\n",
    "To start, we must find the *variance of residuals*, or $\\hat{\\sigma}^2$. This statistic is a measurement of the general difference between predicted and actual values of Y:\n",
    "$$\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n u_i^2}{n-2}$$\n",
    "\n",
    "From the formula above, we can see that $\\hat{\\sigma}^2$ increases as the size of the residuals increase and decreases as our sample size, n, increases. We can then calculate the variance and standard error for the estimate of the slope value using the formula below:\n",
    "$$var(\\beta) = \\frac{\\hat{\\sigma}^2}{\\sum_{i=1}^n (X_i-\\overline{X})^2}$$  \n",
    "\n",
    "$$se(\\beta) = \\sqrt{var(\\beta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Null Hypothesis\n",
    "Once we have the standard error, the procuedures to do hypothesis testing are exactly the same as with difference of means tests.\n",
    "\n",
    "Our null hypothesis is that the slope is zero. So the t-statistic:\n",
    "$$t_\\beta = \\frac{\\beta}{se(\\beta)}$$\n",
    "tells us \"how many standard errors away from zero is our esimate\"\n",
    "\n",
    "Once we have calculated the t-statistic, we can use an online table (or in the appendix of your textbook) to find the p-value for whether there is a statistically significant relationship between X and Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python\n",
    "To find the p-value for the slope of our model, we use <code>stats.linregress(X, Y)</code>. As we saw earlier, this returns the standard error as well, so we can find that using the same function. To access either value, index into the fourth value of the result for the p value, and the fifth value for the standard error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinregressResult(slope=0.6882635096164225, intercept=77.22256835334136, rvalue=0.6461317839588818, pvalue=2.7586432526813464e-05, stderr=0.1415238870758501)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spend_expect_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull out the standard error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1415238870758501"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_b = spend_expect_result[4]\n",
    "se_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use it to create a 95% confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4108766909477563  ,  0.9656503282850886 ]\n"
     ]
    }
   ],
   "source": [
    "slope_lower = lm_slope - 1.96*se_b\n",
    "slope_upper = lm_slope + 1.96*se_b\n",
    "print(\"[\", slope_lower, \" , \", slope_upper, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Interactive Visual\n",
    "Below, we plot the relationship between two random variables. What happens to the results of the OLS model as you change the covariance? When you change the randomness of Y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7b7aa5c5984c48bff4ebd78743c546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='covariance', max=1.0, min=-1.0), IntSlider(value=5, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.cloud(covariance, rand_factor)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cloud(covariance, rand_factor):\n",
    "    X,Y = list(zip(*np.random.multivariate_normal([5,5], [[1,covariance],[covariance,1]], size=1000).tolist()))\n",
    "    Y = Y+np.random.random(1000)*rand_factor\n",
    "    sns.scatterplot(x=X, y=Y)\n",
    "    print(stats.linregress(X,Y))\n",
    "    print(\"R-value:\",stats.pearsonr(X,Y)[0])\n",
    "\n",
    "interact(cloud, covariance=(-1.0,1.0), rand_factor=(0,10), continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [OPTIONAL] Goodness of Fit with non-Linear models\n",
    "Now that we have our linear model, we want to evaulate how well it tracks the relationship between the independent and dependent variables (X and Y). Below, which models are fit well by a linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Generic Plotting function for some f(x)\n",
    "def plot_func(f, label):\n",
    "    x = np.random.randint(1,25, size=50)\n",
    "    y = f(x)\n",
    "    sns.scatterplot(x=x, y=y, label=label)\n",
    "    result = stats.linregress(x, y)\n",
    "    slope = result[0]\n",
    "    intercept = result[1]\n",
    "    plt.plot(x, x * slope + intercept, label='OLS model')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2*x\n",
    "plot_func(f, 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return 4*x**2 + np.random.random(50)\n",
    "plot_func(f, 'quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return  8*np.log(x) + np.random.random(50)\n",
    "plot_func(f, 'logarithmic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, linear models can better represent some relationships than others! We need some measure to quantify if a linear model is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $R^2$  Statistic\n",
    "In order to determine if our linear model is a good fit, we need to understand how much of the variance in the values of Y can be captured in our model. In the 3 plots above, a linear model had differing abilities to represent that variation.  \n",
    "*Review*: Which functions variation was best captured by a linear model?  \n",
    "\n",
    "We need a measurement that compares the overall variation in Y values with the variation that is not represented by our OLS model. This second kind is the *residual variation* because it is the difference between true values of Y and those predicted by our model. The $R^2$ statistic is a value between 0 and 1 that represents the proportion of variation in our Y value that can be accounted for by our model:\n",
    "$$R^2 = 1-\\frac{\\text{residual variation}}{\\text{total variation}}$$\n",
    "The residual variation, or Residual Sum of Squares (RSS), is calculated by adding all squared residuals between the predicted and true values of Y:\n",
    "$$RSS = \\sum_{i=1}^n u_i^2 = \\sum_{i=1}^n (Y_i - \\hat{Y_i})^2$$\n",
    "The total variation, or Total Sum of Squares (TSS), is calculated by adding the squared differences between each value of Y and the mean value of Y:\n",
    "$$TSS = \\sum_{i=1}^n (Y_i - \\overline{Y})^2$$\n",
    "Therefore, the $R^2$ statistic can be found thus:\n",
    "$$R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^n (Y_i - \\hat{Y_i})^2}{\\sum_{i=1}^n (Y_i - \\overline{Y})^2}$$\n",
    "\n",
    "There is another formula for calculating $R^2$ using the Model Sum of Squares (MSS). The Model Sum of Squares is the total squared difference between predicted values of Y and the average value of Y:\n",
    "$$MSS = \\sum_{i=1}^n (\\hat{Y_i} - \\overline{Y})^2$$\n",
    "The formula for $R^2$ using MSS is:\n",
    "$$R^2 = \\frac{MSS}{TSS} = \\frac{\\sum_{i=1}^n (\\hat{Y_i}-\\overline{Y})^2}{\\sum_{i=1}^n (Y_i - \\overline{Y})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python\n",
    "There are 2 ways to calculate the $R^2$ value using Python. We will show both and demonstrate that they return the same result:  \n",
    "1) Write functions calculating the RSS and TSS, and then use those to calculate the $R^2$ statistic (we have done this below  \n",
    "2) Run <code>stats.linregres(X, Y)</code> and square the third value that it returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4174862822418868"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method 1:\n",
    "def rss(y, y_pred):\n",
    "    \"\"\"Return the Residual Sum of Squares between y and y_pred, the predicted values of y\"\"\"\n",
    "    return sum((y - y_pred)**2)\n",
    "def tss(y):\n",
    "    \"\"\"Return the Total Sum of Squares for y\"\"\"\n",
    "    avg = np.mean(y)\n",
    "    return sum((y - avg)**2)\n",
    "\n",
    "def mss(y, y_pred):\n",
    "    \"\"\"Return the Model Sum of Squares for y and y_pred, the predicted values of y\"\"\"\n",
    "    avg = np.mean(y)\n",
    "    return sum((y_pred-avg)**2)\n",
    "\n",
    "def r2(y, y_pred):\n",
    "    \"\"\"Return the R-squared statistic for y and the predicted values of y\"\"\"\n",
    "    return 1 - (rss(y, y_pred) / tss(y))\n",
    "\n",
    "predicted_y = spend_noUS.Spending*lm_slope + lm_intercept\n",
    "r2(spend_noUS.Expectancy, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4174862822418871"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method 2:\n",
    "spend_expect_result[2]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the two results are the same! This is where the name R^2 comes from. Exactly why these end up being the same is beyond our scope; for a formal proof, follow [this link](https://economictheoryblog.com/2014/11/05/proof/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
